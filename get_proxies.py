# V 1.0
'''
æ”¯æŒhttp/https/socks4/socks5

åŠŸèƒ½ä»‹ç»:
æœ¬ç¨‹åºç”¨äºä»£ç†ç®¡ç†,æœ‰ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½:
1.åŠ è½½å’ŒéªŒè¯æ–°ä»£ç†,å¯ä»çˆ¬è™«(è‡ªåŠ¨),æœ¬åœ°æ–‡ä»¶(ç”¨äºæ‰‹åŠ¨æ·»åŠ ä»£ç†æ—¶ä½¿ç”¨,å¯ä»¥é€‰æ‹©ä»£ç†ç±»å‹(è¿™æ ·æ¯”è¾ƒå¿«),ä¹Ÿå¯ç”¨è‡ªåŠ¨æ£€æµ‹(è‹¥ç”¨è‡ªåŠ¨æ£€æµ‹å¯èƒ½è¾ƒæ…¢))åŠ è½½,å¹¶å°†é€šè¿‡çš„ä»£ç†æ·»åŠ åˆ°ä»£ç†æ± æ–‡ä»¶(OUTPUT_FILE).æ–°ä»£ç†ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹ç±»å‹.ä¸ºäº†ä½¿æ¯ä¸ªä»£ç†å¾—åˆ°å……åˆ†ä½¿ç”¨,æœ¬ç¨‹åºé‡‡å–æ–°ä»£ç†éªŒè¯ä¸¤éçš„ç­–ç•¥,åªè¦æœ‰ä¸€æ¬¡é€šè¿‡å³ç®—é€šè¿‡,ç¡®ä¿ä¸ä¼šè½ä¸‹ä»»ä½•ä¸€ä¸ªå¯ç”¨ä»£ç†.
    åœ¨éªŒè¯ä¹‹å‰ä¼šå…ˆå°†é‡å¤ä»£ç†,é”™è¯¯ä»£ç†ç­›é™¤,ç¡®ä¿ä¸åšæ— ç”¨åŠŸ.æ»¡åˆ†100åˆ†,æœ‰æ•ˆä»£ç†éªŒè¯æˆåŠŸé»˜è®¤åˆ†æ•°98åˆ†,è¶…æ—¶ä»£ç†80åˆ†,é”™è¯¯ä»£ç†å’Œæ— æ•ˆä»£ç†0åˆ†(ä¼šè¢«0åˆ†æ¸…é™¤å‡½æ•°æ¸…é™¤).æœ‰ä¸­æ–­æ¢å¤åŠŸèƒ½,å½“éªŒè¯è¿‡ç¨‹è¢«ä¸­æ–­æ—¶,ä¼šè‡ªåŠ¨ä¿å­˜å·²å®Œæˆçš„ä»£ç†åˆ°ä»£ç†æ± ,æœªå®Œæˆçš„ä»£ç†ä¿å­˜åˆ°ä¸­æ–­æ–‡ä»¶,ä¸‹æ¬¡å¯é€‰æ‹©ç»§ç»­éªŒè¯
2.æ£€éªŒå’Œæ›´æ–°ä»£ç†æ± å†…ä»£ç†çš„æœ‰æ•ˆæ€§,ä½¿ç”¨ä»£ç†æ± æ–‡ä»¶ä¸­çš„Typeä½œä¸ºç±»å‹,å†æ¬¡éªŒè¯æˆåŠŸåŠ 1åˆ†,è¶…æ—¶ä»£ç†åˆ†ä¸å˜,æ— æ•ˆä»£ç†å’Œé”™è¯¯ä»£ç†å‡1åˆ†,æ›´ç›´è§‚çš„åˆ†è¾¨ä»£ç†çš„ç¨³å®šæ€§.æœ‰ä¸­æ–­æ¢å¤åŠŸèƒ½,å½“éªŒè¯è¿‡ç¨‹è¢«ä¸­æ–­æ—¶,ä¼šè‡ªåŠ¨ä¿å­˜å·²å®Œæˆçš„ä»£ç†åˆ°ä»£ç†æ± ,æœªå®Œæˆçš„ä»£ç†ä¿å­˜åˆ°ä¸­æ–­æ–‡ä»¶,ä¸‹æ¬¡å¯é€‰æ‹©ç»§ç»­éªŒè¯
3.æå–æŒ‡å®šæ•°é‡çš„ä»£ç†,ä¼˜å…ˆæå–åˆ†æ•°é«˜,ç¨³å®šçš„ä»£ç†,å¯æŒ‡å®šæå–ç±»å‹
4.æŸ¥çœ‹ä»£ç†æ± çŠ¶æ€(æ€»ä»£ç†æ•°é‡,å„ç§ç±»å‹ä»£ç†çš„åˆ†æ•°åˆ†å¸ƒæƒ…å†µ)

ä»£ç†æ± æ–‡ä»¶æ ¼å¼(OUTPUT_FILE) -> csv
Type,Proxy:Port,Score
ç±»å‹,ä»£ç†:ç«¯å£,åˆ†æ•°

å»ºè®®å°†æ­¤ç¨‹åºæ”¾åˆ°proxiesæ–‡ä»¶å¤¹ä¸‹,å¦‚æœæƒ³æ”¾åˆ°å…¶ä»–åœ°æ–¹,è¯·ä¿®æ”¹'OUTPUT_FILE = "../proxies/valid_proxies.csv"'å’Œ'INTERRUPT_DIR = "../proxies/interrupt"'ä¸­çš„proxiesä¸ºä½ çš„æ–‡ä»¶å¤¹åç§°
'''

import re
import requests
import concurrent.futures
import time
import os
import sys
import csv
import signal

# ============é»˜è®¤é…ç½®åŒº - Default Configuration
OUTPUT_FILE = "../proxies/valid_proxies.csv"  # è¾“å‡ºæœ‰æ•ˆä»£ç†æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰- Export valid proxy file (CSV format)
TEST_URL = "http://httpbin.org/ip"  # æµ‹è¯•ä½¿ç”¨çš„URL - URL used for testing
TIMEOUT = 6  # è¶…æ—¶æ—¶é—´(ç§’) - Timeout (s)
MAX_WORKERS = 80  # æœ€å¤§å¹¶å‘æ•° - Maximum concurrency
MAX_SCORE = 100  # æœ€å¤§ç§¯åˆ† - Maximum score

# ä¸­æ–­æ¢å¤ç›¸å…³é…ç½®
INTERRUPT_DIR = "../proxies/interrupt"  # ä¸­æ–­æ–‡ä»¶ç›®å½•
INTERRUPT_FILE = os.path.join(INTERRUPT_DIR, "interrupted_proxies.csv")  # çˆ¬å–éªŒè¯ä¸­æ–­æ–‡ä»¶
INTERRUPT_FILE_LOAD = os.path.join(INTERRUPT_DIR, "interrupted_load_proxies.csv")   # æœ¬åœ°æ–‡ä»¶åŠ è½½ä¸­æ–­æ–‡ä»¶
INTERRUPT_FILE_EXISTING = os.path.join(INTERRUPT_DIR, "interrupted_existing_proxies.csv")   # æ›´æ–°ä»£ç†æ± ä¸­æ–­æ–‡ä»¶

# å…¨å±€å˜é‡ç”¨äºä¸­æ–­å¤„ç†
current_validation_process = None
interrupted = False

def create_interrupt_dir():
    """åˆ›å»ºä¸­æ–­ç›®å½•"""
    os.makedirs(INTERRUPT_DIR, exist_ok=True)

def save_interrupted_proxies(remaining_proxies, proxy_type, original_count, interrupt_file=INTERRUPT_FILE):
    """ä¿å­˜ä¸­æ–­æ—¶çš„ä»£ç†åˆ—è¡¨"""
    create_interrupt_dir()
    with open(interrupt_file, 'w', encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        writer.writerow([proxy_type, original_count])  # ç¬¬ä¸€è¡Œä¿å­˜ç±»å‹å’ŒåŸå§‹æ•°é‡
        for proxy in remaining_proxies:
            writer.writerow([proxy])

def load_interrupted_proxies(interrupt_file=INTERRUPT_FILE):
    """åŠ è½½ä¸­æ–­çš„ä»£ç†åˆ—è¡¨"""
    # å¦‚æœæ²¡æœ‰ä¸­æ–­è®°å½•
    if not os.path.exists(interrupt_file):
        return None, None, None
    
    try:
        with open(interrupt_file, 'r', encoding="utf-8") as file:
            reader = csv.reader(file)
            first_row = next(reader, None)
            # å¦‚æœæ— æ•ˆ
            if not first_row or len(first_row) < 2:
                return None, None, None
            
            proxy_type = first_row[0]
            original_count = int(first_row[1])
            remaining_proxies = [row[0] for row in reader if row]
        # æœ‰æ•ˆå¹¶æˆåŠŸè¯»å–  
        return remaining_proxies, proxy_type, original_count  # å‰©ä½™ä»£ç†,ç±»å‹,åŸå§‹æ•°é‡
    # å¤±è´¥
    except:
        return None, None, None

def delete_interrupt_file(interrupt_file=INTERRUPT_FILE):
    """åˆ é™¤ä¸­æ–­æ–‡ä»¶"""
    if os.path.exists(interrupt_file):
        os.remove(interrupt_file)

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºæ•è·Ctrl+C"""
    global interrupted
    interrupted = True
    print("\n\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")

def setup_interrupt_handler():
    """è®¾ç½®ä¸­æ–­å¤„ç†å™¨"""
    global interrupted
    interrupted = False
    signal.signal(signal.SIGINT, signal_handler)

class ProxyScraper:
    """
    get ip

    :param url: è¯·æ±‚åœ°å€
    :param regex_pattern: reè§£æå¼ï¼Œç”¨äºè§£æçˆ¬å–ç»“æœ
    :param capture_groups: è¦è¿”å›çš„reä¸­çš„å€¼ï¼Œ[IpName,Port]
    :return: [proxy:port]
    """
    def __init__(self, url: str, regex_pattern: str, capture_groups: list):
        self.url = url
        self.headers = {
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0'
        }
        self.encoding = "utf-8"
        self.regex_pattern = regex_pattern
        self.capture_groups = capture_groups

    def scrape_proxies(self):
        extracted_data = []
        try:
            response = requests.get(url=self.url, headers=self.headers, timeout=TIMEOUT)
            if response.status_code == 200:  # åˆ¤æ–­çŠ¶æ€ç 
                response.encoding = self.encoding  # ä½¿ç”¨utf-8
                regex = re.compile(self.regex_pattern, re.S)  # åˆ›å»ºä¸€ä¸ªreå¯¹è±¡
                matches = regex.finditer(response.text)  # å¯¹è·å–çš„ä¸œè¥¿è¿›è¡Œè§£æ
                for match in matches:
                    for group_name in self.capture_groups:  # ä¾æ¬¡è¾“å‡ºå‚æ•°capture_groupsä¸­çš„æŒ‡å®šå†…å®¹
                        extracted_data.append(f"{match.group(group_name)}")
                proxy_list = [f"{extracted_data[i].strip()}:{extracted_data[i + 1].strip()}" for i in
                            range(0, len(extracted_data), 2)]  # æ•´åˆåˆ—è¡¨ä¸º[proxy:port]
                response.close()
                return proxy_list
            else:
                get_error = f"\nçˆ¬å–å¤±è´¥ï¼ŒâŒ çŠ¶æ€ç {response.status_code}"   # å‰é¢çš„\né˜²æ­¢ä¸è¿›åº¦æ¡æ··åœ¨ä¸€è¡Œ
                print(get_error)
                return get_error

        except Exception as e:
            get_error = f"\nçˆ¬å–å¤±è´¥ï¼ŒâŒ é”™è¯¯: {str(e)}"
            print(get_error)
            return get_error

def check_proxy(proxy, test_url="http://httpbin.org/ip", timeout=TIMEOUT, 
                retries=1, proxy_type="auto"):
    """
    æ£€æŸ¥å•ä¸ªä»£ç†IPçš„å¯ç”¨æ€§ï¼ˆæ”¯æŒHTTPå’ŒSOCKSï¼‰
    
    :param proxy_type: ä»£ç†ç±»å‹ - "auto"(è‡ªåŠ¨æ£€æµ‹), "http", "socks4", "socks5"
    :param proxy: ä»£ç†IPåœ°å€å’Œç«¯å£ (æ ¼å¼: ip:port)
    :param test_url: ç”¨äºæµ‹è¯•çš„URL
    :param timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
    :param retries: é‡è¯•æ¬¡æ•°
    :return: ä»£ç†åœ°å€, æ˜¯å¦å¯ç”¨, å“åº”æ—¶é—´, ä»£ç†ç±»å‹
    """
    # æ ¹æ®ä»£ç†ç±»å‹è®¾ç½®proxieså­—å…¸
    if proxy_type == "auto":
        # è‡ªåŠ¨æ£€æµ‹ï¼šå…ˆå°è¯•HTTPï¼Œå†å°è¯•SOCKS5ï¼Œæœ€åSOCKS4
        protocols_to_try = ["http", "socks5", "socks4"]
    else:
        # æŒ‡å®šç±»å‹æ—¶ï¼Œåªå°è¯•è¯¥ç±»å‹
        protocols_to_try = [proxy_type]
    
    detected_type = proxy_type if proxy_type != "auto" else "unknown"
    
    for current_protocol in protocols_to_try:
        if current_protocol == "http":
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        elif current_protocol == "socks4":
            proxies_config = {
                "http": f"socks4://{proxy}",
                "https": f"socks4://{proxy}"
            }
        elif current_protocol == "socks5":
            proxies_config = {
                "http": f"socks5://{proxy}",
                "https": f"socks5://{proxy}"
            }
        else:
            continue

        for attempt in range(retries):
            try:
                start_time = time.time()
                response = requests.get(
                    test_url,
                    proxies=proxies_config,
                    timeout=timeout,
                    allow_redirects=False
                )
                end_time = time.time()
                response_time = end_time - start_time

                if response_time > timeout:
                    # è¶…æ—¶ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªåè®®ï¼ˆå¦‚æœæ˜¯è‡ªåŠ¨æ£€æµ‹ï¼‰
                    break

                if response.status_code == 200:
                    origin_ip = response.json().get('origin', '')
                    # éªŒè¯è¿”å›çš„IPæ˜¯å¦ä¸ä»£ç†ä¸€è‡´
                    proxy_ip = proxy.split(':')[0]
                    if proxy_ip in origin_ip:
                        detected_type = current_protocol
                        return proxy, True, response_time, detected_type
                    detected_type = current_protocol
                    return proxy, True, response_time, detected_type
                    
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(0.5)
                    continue
                # å½“å‰åè®®å¤±è´¥ï¼Œå¦‚æœæ˜¯è‡ªåŠ¨æ£€æµ‹åˆ™å°è¯•ä¸‹ä¸€ä¸ªåè®®
                break
                
    # å¦‚æœæ˜¯æŒ‡å®šç±»å‹éªŒè¯å¤±è´¥ï¼Œè¿”å›æŒ‡å®šç±»å‹ï¼ˆå³ä½¿å¤±è´¥ï¼‰
    if proxy_type != "auto":
        detected_type = proxy_type
    
    return proxy, False, None, detected_type

def check_proxies_batch(proxies, proxy_types, test_url="http://httpbin.org/ip", 
                       timeout=TIMEOUT, max_workers=MAX_WORKERS, check_type="existing"):
    """
    æ‰¹é‡æ£€æŸ¥ä»£ç†IPåˆ—è¡¨
    
    :param proxy_types: ä»£ç†ç±»å‹å­—å…¸
    """
    global interrupted
    
    updated_proxies = {}
    updated_types = {}

    # æ–°ä»£ç†éªŒè¯ä¸¤æ¬¡ï¼Œå·²æœ‰ä»£ç†éªŒè¯ä¸€æ¬¡
    retries = 2 if check_type == "new" else 1

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {}
        for proxy in proxies:
            if interrupted:
                break
                
            # å¯¹äºå·²æœ‰ä»£ç†ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­è®°å½•çš„ç±»å‹ï¼›å¯¹äºæ–°ä»£ç†ï¼Œä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
            if check_type == "existing" and proxy in proxy_types:
                proxy_type = proxy_types[proxy]
            else:
                proxy_type = proxy_types.get(proxy, "auto")  # ä»ä¼ å…¥çš„ç±»å‹å­—å…¸è·å–
                
            future = executor.submit(
                check_proxy,
                proxy,
                test_url,
                timeout,
                retries,
                proxy_type
            )
            future_to_proxy[future] = proxy

        for future in concurrent.futures.as_completed(future_to_proxy):
            if interrupted:
                # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                for f in future_to_proxy:
                    f.cancel()
                break
                
            proxy = future_to_proxy[future]
            try:
                proxy_addr, is_valid, response_time, detected_type = future.result()

                if is_valid and response_time is not None and response_time <= timeout:
                    print(f"âœ… æœ‰æ•ˆä»£ç†({detected_type}): {proxy} | å“åº”æ—¶é—´: {response_time:.2f}s")
                    if check_type == "new":
                        updated_proxies[proxy] = 98
                    else:
                        current_score = proxies.get(proxy, 0)
                        updated_proxies[proxy] = min(current_score + 1, MAX_SCORE)
                    updated_types[proxy] = detected_type
                    
                elif response_time is not None:
                    print(f"â è¶…æ—¶ä»£ç†: {proxy} | å“åº”æ—¶é—´: {response_time:.2f}s")
                    if check_type == "existing" and proxy in proxies:
                        updated_proxies[proxy] = proxies[proxy]
                    else:
                        updated_proxies[proxy] = 80
                    # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿä¿ç•™æŒ‡å®šçš„ç±»å‹
                    updated_types[proxy] = proxy_types.get(proxy, detected_type)

                else:
                    print(f"âŒ æ— æ•ˆä»£ç†: {proxy}")
                    if check_type == "existing" and proxy in proxies:
                        updated_proxies[proxy] = max(0, proxies[proxy] - 1)
                        updated_types[proxy] = proxy_types.get(proxy, "http")
                    else:
                        updated_proxies[proxy] = 0
                        updated_types[proxy] = proxy_types.get(proxy, "http")
                        
            except Exception as e:
                if not interrupted:  # åªæœ‰ä¸æ˜¯ä¸­æ–­å¼•èµ·çš„å¼‚å¸¸æ‰æ‰“å°
                    print(f"âŒ é”™è¯¯ä»£ç†: {proxy} - {str(e)}")
                if check_type == "existing" and proxy in proxies:
                    updated_proxies[proxy] = max(0, proxies[proxy] - 1)
                    updated_types[proxy] = proxy_types.get(proxy, "http")
                else:
                    updated_proxies[proxy] = 0
                    updated_types[proxy] = proxy_types.get(proxy, "http")
                    
    return updated_proxies, updated_types

def load_proxies_from_file(file_path):
    """ä»CSVæ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨ã€ç±»å‹å’Œåˆ†æ•°"""
    proxies = {}
    proxy_types = {}
    
    if not os.path.exists(file_path):
        return proxies, proxy_types

    with open(file_path, 'r', encoding="utf-8") as file:
        reader = csv.reader(file)
        for row in reader:
            if len(row) >= 3:
                # æ–°æ ¼å¼ï¼šç±»å‹,proxy:port,åˆ†æ•°
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"  # é»˜è®¤HTTP
            elif len(row) >= 2:
                # æ—§æ ¼å¼å…¼å®¹ï¼šproxy:port,åˆ†æ•°ï¼ˆé»˜è®¤HTTPç±»å‹ï¼‰
                proxy = row[0].strip()
                try:
                    score = int(row[1])
                    proxies[proxy] = score
                    proxy_types[proxy] = "http"  # æ—§æ–‡ä»¶é»˜è®¤HTTP
                except:
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
    
    return proxies, proxy_types

def save_valid_proxies(proxies, proxy_types, file_path):
    """ä¿å­˜æœ‰æ•ˆä»£ç†åˆ°CSVæ–‡ä»¶ï¼ˆå¸¦ç±»å‹å’Œåˆ†æ•°ï¼‰"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    with open(file_path, 'w', encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        for proxy, score in proxies.items():
            if len(proxy) > 6 and score > 0:  # åŸºæœ¬éªŒè¯
                proxy_type = proxy_types.get(proxy, "http")  # é»˜è®¤HTTP
                writer.writerow([proxy_type, proxy, score])

def update_proxy_scores(file_path):
    """æ›´æ–°ä»£ç†åˆ†æ•°æ–‡ä»¶ï¼Œç§»é™¤0åˆ†ä»£ç†"""
    proxies, proxy_types = load_proxies_from_file(file_path)
    valid_proxies = {k: v for k, v in proxies.items() if v > 0}
    valid_types = {k: v for k, v in proxy_types.items() if k in valid_proxies}
    save_valid_proxies(valid_proxies, valid_types, file_path)
    return len(proxies) - len(valid_proxies)

def filter_proxies(all_proxies):
        """
        ä»æ–°è·å–ä»£ç†ä¸­å»æ‰æ— æ•ˆçš„,é‡å¤çš„
        :all_proxies: æ–°ä»£ç†åˆ—è¡¨
        :return: ç­›é€‰åçš„ä»£ç†åˆ—è¡¨
        """
        # è¿›è¡Œç­›é€‰
        existing_proxies = []
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE,'r') as file:
                csv_reader  = csv.reader(file)
                for row in csv_reader:
                    existing_proxies.append(row[1])

        new_proxies = []
        duplicate_count = 0
        invalid_count = 0

        for proxy in all_proxies:
            try:
                if (proxy in existing_proxies) or (proxy in new_proxies):
                    print(f'â­•ï¸ å·²æœ‰ä»£ç†: {proxy}')
                    duplicate_count += 1
                elif (':' in proxy) and (proxy not in new_proxies):
                    new_proxies.append(proxy)
                    
                else:
                    print(f'âŒ æ ¼å¼æ— æ•ˆ: {proxy}')
                    invalid_count += 1
            except:
                invalid_count += 1

        print(f'æ–°ä»£ç†:{len(new_proxies)},å·²æœ‰(é‡å¤):{duplicate_count},æ— æ•ˆ:{invalid_count}')
        return new_proxies

def validate_new_proxies_with_interrupt(new_proxies, proxy_type="auto", from_interrupt=False, source="crawl"):
    """éªŒè¯æ–°ä»£ç†ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤ï¼‰"""
    global interrupted
    
    if not new_proxies:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    # æ ¹æ®æ¥æºé€‰æ‹©ä¸­æ–­æ–‡ä»¶
    interrupt_file = INTERRUPT_FILE if source == "crawl" else INTERRUPT_FILE_LOAD
    
    original_count = len(new_proxies)
    print(f"å…±åŠ è½½ {original_count} ä¸ªæ–°ä»£ç†ï¼Œä½¿ç”¨{proxy_type}ç±»å‹å¼€å§‹æµ‹è¯•...")
    
    # ä¿å­˜åˆå§‹çŠ¶æ€åˆ°ä¸­æ–­æ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯ä»ä¸­æ–­æ¢å¤çš„ï¼‰
    if not from_interrupt:
        save_interrupted_proxies(new_proxies, proxy_type, original_count, interrupt_file)
        print(f"ğŸ“ å·²åˆ›å»ºä¸­æ–­æ¢å¤æ–‡ä»¶: {interrupt_file}")
    
    # è®¾ç½®ä¸­æ–­å¤„ç†å™¨
    setup_interrupt_handler()
    
    # æ–°ä»£ç†åˆå§‹åˆ†æ•°ä¸º0
    new_proxies_dict = {proxy: 0 for proxy in new_proxies}
    new_types_dict = {proxy: proxy_type for proxy in new_proxies}
    
    try:
        updated_proxies, updated_types = check_proxies_batch(
            new_proxies_dict, new_types_dict, TEST_URL, TIMEOUT, 
            MAX_WORKERS, check_type="new"
        )
        
        if interrupted:
            # è®¡ç®—å‰©ä½™æœªéªŒè¯çš„ä»£ç†
            verified_proxies = set(updated_proxies.keys())
            remaining_proxies = [proxy for proxy in new_proxies if proxy not in verified_proxies]
            
            # ä¿å­˜å·²éªŒè¯çš„ä»£ç†åˆ°ä»£ç†æ± 
            existing_proxies, existing_types = load_proxies_from_file(OUTPUT_FILE)
            for proxy, score in updated_proxies.items():
                if proxy not in existing_proxies or existing_proxies[proxy] < score:
                    existing_proxies[proxy] = score
                    existing_types[proxy] = updated_types[proxy]

            save_valid_proxies(existing_proxies, existing_types, OUTPUT_FILE)
            
            # æ›´æ–°ä¸­æ–­æ–‡ä»¶
            if remaining_proxies:
                save_interrupted_proxies(remaining_proxies, proxy_type, original_count, interrupt_file)
                print(f"\nâ¸ï¸ éªŒè¯å·²ä¸­æ–­ï¼å·²ä¿å­˜ {len(verified_proxies)} ä¸ªä»£ç†åˆ°ä»£ç†æ± ï¼Œå‰©ä½™ {len(remaining_proxies)} ä¸ªä»£ç†å¾…éªŒè¯")
                print(f"ğŸ“ ä¸­æ–­æ–‡ä»¶å·²æ›´æ–°: {interrupt_file}")
            else:
                delete_interrupt_file(interrupt_file)
                print(f"\nâœ… éªŒè¯å®Œæˆï¼æ‰€æœ‰ä»£ç†å·²éªŒè¯å¹¶ä¿å­˜")
                
            interrupted = False
            return
        
        # æ­£å¸¸å®ŒæˆéªŒè¯
        # åˆå¹¶åˆ°ç°æœ‰ä»£ç†æ± 
        existing_proxies, existing_types = load_proxies_from_file(OUTPUT_FILE)
        for proxy, score in updated_proxies.items():
            if proxy not in existing_proxies or existing_proxies[proxy] < score:
                existing_proxies[proxy] = score
                existing_types[proxy] = updated_types[proxy]

        save_valid_proxies(existing_proxies, existing_types, OUTPUT_FILE)
        
        # åˆ é™¤ä¸­æ–­æ–‡ä»¶
        delete_interrupt_file(interrupt_file)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for score in updated_proxies.values() if score == 98)
        timeout_count = sum(1 for score in updated_proxies.values() if score == 80)
        
        print(f"\nâœ… éªŒè¯å®Œæˆ!")
        print(f"æˆåŠŸ: {success_count}/{original_count}")
        print(f"è¶…æ—¶: {timeout_count}/{original_count}")
        print(f"ä»£ç†æ± å·²æ›´æ–°è‡³: {OUTPUT_FILE}")
        
    except Exception as e:
        if not interrupted:
            print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def validate_existing_proxies_with_interrupt():
    """éªŒè¯å·²æœ‰ä»£ç†æ± ä¸­çš„ä»£ç†ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤ï¼‰"""
    global interrupted
    
    print(f"å¼€å§‹éªŒè¯å·²æœ‰ä»£ç†æ± ï¼Œæ–‡ä»¶ï¼š{OUTPUT_FILE}...")
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è®°å½•
    remaining_proxies, _, original_count = load_interrupted_proxies(INTERRUPT_FILE_EXISTING)
    if remaining_proxies:
        print(f"ğŸ” å‘ç°ä¸Šæ¬¡éªŒè¯ä¸­æ–­è®°å½•!")
        print(f"   å‰©ä½™ä»£ç†: {len(remaining_proxies)}/{original_count} ä¸ª")
        print("\nè¯·é€‰æ‹©:")
        print("  y: ç»§ç»­ä¸Šæ¬¡éªŒè¯")
        print("  n: åˆ é™¤è®°å½•å¹¶é‡æ–°éªŒè¯")
        print("  å…¶ä»–: è¿”å›ä¸Šçº§èœå•")
        
        choice = input("è¯·é€‰æ‹© (y/n/å…¶ä»–): ").lower().strip()
        
        if choice == 'y':
            print("ç»§ç»­ä¸Šæ¬¡éªŒè¯...")
            proxies_to_validate = remaining_proxies
        elif choice == 'n':
            delete_interrupt_file(INTERRUPT_FILE_EXISTING)
            proxies_to_validate = None  # é‡æ–°åŠ è½½æ‰€æœ‰ä»£ç†
        else:
            print("è¿”å›ä¸Šçº§èœå•")
            return
    else:
        proxies_to_validate = None
    
    # åŠ è½½ä»£ç†æ± 
    all_proxies, proxy_types = load_proxies_from_file(OUTPUT_FILE)
    
    if proxies_to_validate is None:
        # é‡æ–°éªŒè¯æ‰€æœ‰ä»£ç†
        proxies_to_validate = list(all_proxies.keys())
        original_count = len(proxies_to_validate)
    
    if not proxies_to_validate:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    print(f"å…±åŠ è½½ {len(proxies_to_validate)} ä¸ªä»£ç†ï¼Œå¼€å§‹æµ‹è¯•...")
    
    # ä¿å­˜åˆå§‹çŠ¶æ€åˆ°ä¸­æ–­æ–‡ä»¶
    save_interrupted_proxies(proxies_to_validate, "already_have", original_count, INTERRUPT_FILE_EXISTING)
    print(f"ğŸ“ å·²åˆ›å»ºä¸­æ–­æ¢å¤æ–‡ä»¶: {INTERRUPT_FILE_EXISTING}")
    
    # è®¾ç½®ä¸­æ–­å¤„ç†å™¨
    setup_interrupt_handler()
    
    try:
        # ä»ä»£ç†æ± ä¸­è·å–å½“å‰åˆ†æ•°å’Œç±»å‹
        proxies_dict = {proxy: all_proxies[proxy] for proxy in proxies_to_validate}
        types_dict = {proxy: proxy_types[proxy] for proxy in proxies_to_validate}
        
        updated_proxies, updated_types = check_proxies_batch(
            proxies_dict, types_dict, TEST_URL, TIMEOUT, MAX_WORKERS, "existing"
        )
        
        if interrupted:
            # è®¡ç®—å‰©ä½™æœªéªŒè¯çš„ä»£ç†
            verified_proxies = set(updated_proxies.keys())
            remaining_proxies = [proxy for proxy in proxies_to_validate if proxy not in verified_proxies]
            
            # æ›´æ–°å·²éªŒè¯çš„ä»£ç†åˆ†æ•°
            for proxy, score in updated_proxies.items():
                all_proxies[proxy] = score
            
            # ä¿å­˜æ›´æ–°åçš„ä»£ç†æ± 
            save_valid_proxies(all_proxies, proxy_types, OUTPUT_FILE)
            
            # æ›´æ–°ä¸­æ–­æ–‡ä»¶
            if remaining_proxies:
                save_interrupted_proxies(remaining_proxies, "already_have", original_count, INTERRUPT_FILE_EXISTING)
                print(f"\nâ¸ï¸ éªŒè¯å·²ä¸­æ–­ï¼å·²æ›´æ–° {len(verified_proxies)} ä¸ªä»£ç†åˆ†æ•°ï¼Œå‰©ä½™ {len(remaining_proxies)} ä¸ªä»£ç†å¾…éªŒè¯")
                print(f"ğŸ“ ä¸­æ–­æ–‡ä»¶å·²æ›´æ–°: {INTERRUPT_FILE_EXISTING}")
            else:
                delete_interrupt_file(INTERRUPT_FILE_EXISTING)
                print(f"\nâœ… éªŒè¯å®Œæˆï¼æ‰€æœ‰ä»£ç†å·²æ›´æ–°")
                
            interrupted = False
            return
        
        # æ­£å¸¸å®ŒæˆéªŒè¯
        # æ›´æ–°æ‰€æœ‰ä»£ç†åˆ†æ•°
        for proxy, score in updated_proxies.items():
            all_proxies[proxy] = score
        
        # ä¿å­˜æ›´æ–°åçš„ä»£ç†æ± 
        save_valid_proxies(all_proxies, proxy_types, OUTPUT_FILE)
        
        # æ¸…ç†0åˆ†ä»£ç†
        removed_count = update_proxy_scores(OUTPUT_FILE)
        
        # åˆ é™¤ä¸­æ–­æ–‡ä»¶
        delete_interrupt_file(INTERRUPT_FILE_EXISTING)
        
        # æœ€ç»ˆç»Ÿè®¡
        final_proxies, _ = load_proxies_from_file(OUTPUT_FILE)
        final_count = len(final_proxies)

        print(f"\néªŒè¯å®Œæˆ! å‰©ä½™æœ‰æ•ˆä»£ç†: {final_count}/{original_count}")
        print(f"å·²ç§»é™¤ {original_count - final_count} ä¸ªæ— æ•ˆä»£ç†")
        
    except Exception as e:
        if not interrupted:
            print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


def crawl_proxies():
    """çˆ¬å–å…è´¹ä»£ç†ï¼ˆæ·»åŠ ä¸­æ–­æ¢å¤æ£€æŸ¥ï¼‰"""
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è®°å½•
    remaining_proxies, proxy_type, original_count = load_interrupted_proxies(INTERRUPT_FILE)
    if remaining_proxies:
        print(f"ğŸ” å‘ç°ä¸Šæ¬¡ä¸­æ–­è®°å½•!")
        print(f"   å‰©ä½™ä»£ç†: {len(remaining_proxies)}/{original_count} ä¸ª")
        print(f"   éªŒè¯ç±»å‹: {proxy_type}")
        print("\nè¯·é€‰æ‹©:")
        print("  y: ç»§ç»­ä¸Šæ¬¡éªŒè¯")
        print("  n: åˆ é™¤è®°å½•å¹¶é‡æ–°çˆ¬å–")
        print("  å…¶ä»–: è¿”å›ä¸Šçº§èœå•")
        
        choice = input("è¯·é€‰æ‹© (y/n/å…¶ä»–): ").lower().strip()
        
        if choice == 'y':
            print("ç»§ç»­ä¸Šæ¬¡éªŒè¯...")
            return remaining_proxies, proxy_type
        elif choice == 'n':
            delete_interrupt_file(INTERRUPT_FILE)
            print("å·²åˆ é™¤ä¸­æ–­è®°å½•ï¼Œå¼€å§‹é‡æ–°çˆ¬å–...")
        else:
            print("è¿”å›ä¸Šçº§èœå•")
            return None, None

    print("""å·²åˆ›å»ºçš„å¯çˆ¬ç½‘ç«™
    1 ï¼šhttps://proxy5.net/cn/free-proxy/china
          å¤‡æ³¨:è¢«å°äº†,æˆåŠŸç‡ 40%
    2 ï¼šhttps://www.89ip.cn/
          å¤‡æ³¨:240ä¸ª,æˆåŠŸç‡ 10%
    3 ï¼šhttps://cn.freevpnnode.com/
          å¤‡æ³¨:30ä¸ª,æˆåŠŸç‡ 3%
    4 ï¼šhttps://www.kuaidaili.com/free/inha/ 
          å¤‡æ³¨:7600å¤šé¡µ,æˆåŠŸç‡ 5%
    5 ï¼šhttp://www.ip3366.net/
          å¤‡æ³¨:100ä¸ª,æˆåŠŸç‡ 1%
    6 ï¼šhttps://proxypool.scrape.center/random
          å¤‡æ³¨:éšæœºçš„,æˆåŠŸç‡ 40%
    7 ï¼šhttps://proxy.scdn.io/text.php
          å¤‡æ³¨:12000å¤šä¸ª,æˆåŠŸç‡ 30%
    8 ï¼šhttps://proxyhub.me/zh/cn-http-proxy-list.html
          å¤‡æ³¨:20ä¸ª,æˆåŠŸç‡ 0%
    9 : https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/http.txt
          å¤‡æ³¨:å¤§çº¦3000ä¸ª,æˆåŠŸç‡ 15%
    10: https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/socks5.txt
          å¤‡æ³¨:å¤§çº¦2000ä¸ª,æˆåŠŸç‡ 10%
    11: https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/https.txt
          å¤‡æ³¨:å¤§çº¦3000ä¸ª,æˆåŠŸç‡ 10%
          
    è¾“å…¥å…¶ä»–ï¼šé€€å‡º
    """)
    scraper_choice = input("é€‰æ‹©ï¼š").strip()
    all_proxies = []  # å­˜å‚¨æ‰€æœ‰çˆ¬å–çš„ä»£ç†
    by_type = ''  # é€šè¿‡æŒ‡å®šç±»å‹éªŒè¯,é»˜è®¤ä¸ºå¦

    if scraper_choice == "1":
        print('å¼€å§‹çˆ¬å–:https://proxy5.net/cn/free-proxy/china')
        error_count = 0
        '''
        <tr>.*?<td><strong>(?P<ip>.*?)</strong></td>.*?<td>(?P<port>.*?)</td>.*?</tr>
        '''
        proxy_list = ProxyScraper('https://proxy5.net/cn/free-proxy/china',
                        "<tr>.*?<td><strong>(?P<ip>.*?)</strong></td>.*?<td>(?P<port>.*?)</td>.*?</tr>",
                        ["ip", "port"]).scrape_proxies()
        
        if isinstance(proxy_list, list):
            all_proxies.extend(proxy_list)
        else:
            error_count += 1
        print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')

    elif scraper_choice == "2":
        print('å¼€å§‹çˆ¬å–:https://www.89ip.cn/')
        error_count = 0
        total_pages = 6
        for page in range(1, total_pages+1):
            if page == 1:
                url = 'https://www.89ip.cn/'
            else:
                url = f'https://www.89ip.cn/index_{page}.html'

            proxy_list = ProxyScraper(url,"<tr>.*?<td>(?P<ip>.*?)</td>.*?<td>(?P<port>.*?)</td>.*?</tr>",
                            ["ip", "port"]).scrape_proxies()
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1

            time.sleep(1)

            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            percent = page * 100 // total_pages
            # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
            completed = page * 50 // total_pages
            remaining = 50 - completed
            # å¤„ç†ç™¾åˆ†æ¯”æ˜¾ç¤ºçš„å¯¹é½
            if percent < 10:
                padding = "  "
            elif percent < 100:
                padding = " "
            else:
                padding = ""
            # æ›´æ–°è¿›åº¦æ¡
            print(f"\r{percent}%{padding}|{'â–ˆ' * completed}{'-' * remaining}| {page}/{total_pages}  é”™è¯¯æ•°:{error_count}", end="")
            sys.stdout.flush()
        print('\n')
        
    elif scraper_choice == "3":
        print('\nå¼€å§‹çˆ¬å–:https://cn.freevpnnode.com/')
        error_count = 0
        proxy_list = ProxyScraper("https://cn.freevpnnode.com/",
                        '<tr>.*?<td>(?P<ip>.*?)</td>.*?<td>(?P<port>.*?)</td>.*?<td><span>.*?</span> <img src=".*?" width="20" height="20" .*? class="js_openeyes"></td>.*?</td>',
                        ["ip", "port"]).scrape_proxies()
        if isinstance(proxy_list, list):
            all_proxies.extend(proxy_list)
        else:
            error_count += 1
        print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')

    elif scraper_choice == "4":
        error_count = 0
        try:
            print('ä¿¡æ¯:å…±çº¦7000é¡µ,å»ºè®®ä¸€æ¬¡çˆ¬å–æ•°é‡ä¸å¤§äº500é¡µ,é˜²æ­¢è¢«å°')
            start_page = int(input('çˆ¬å–èµ·å§‹é¡µï¼ˆæ•´æ•°ï¼‰ï¼š').strip())
            end_page = int(input("çˆ¬å–ç»“æŸé¡µï¼ˆæ•´æ•°ï¼‰:").strip())
            if end_page < 1 or start_page < 1 or end_page > 7000 or start_page > 7000 or start_page > end_page:
                print("ä¸èƒ½å°äº1æˆ–å¤§äº7000,èµ·å§‹é¡µä¸èƒ½å¤§äºç»“æŸé¡µ")
                return

            print('å¼€å§‹çˆ¬å–:https://www.kuaidaili.com/free/inha/')
            
            for page in range(start_page, end_page + 1):

                proxy_list = ProxyScraper(f"https://www.kuaidaili.com/free/inha/{page}/",
                                '{"ip": "(?P<ip>.*?)", "last_check_time": ".*?", "port": "(?P<port>.*?)", "speed": .*?, "location": ".*?"}',
                                ["ip", "port"]).scrape_proxies()
                if isinstance(proxy_list, list):
                    all_proxies.extend(proxy_list)
                else:
                    error_count += 1

                time.sleep(2)

                # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
                current_page = page - start_page + 1
                total_pages = end_page - start_page + 1
                percent = current_page * 100 // total_pages
                # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
                completed = current_page * 50 // total_pages
                remaining = 50 - completed
                # å¤„ç†ç™¾åˆ†æ¯”æ˜¾ç¤ºçš„å¯¹é½
                if percent < 10:
                    padding = "  "
                elif percent < 100:
                    padding = " "
                else:
                    padding = ""
                # æ›´æ–°è¿›åº¦æ¡
                print(f"\r{percent}%{padding}|{'â–ˆ' * completed}{'-' * remaining}| {current_page}/{total_pages}  é”™è¯¯æ•°:{error_count}", end="")
                sys.stdout.flush()
            print('\n')
        except:
            print("è¾“å…¥é”™è¯¯ï¼Œè¯·è¾“å…¥æ•´æ•°")

    elif scraper_choice == "5":

        print('\nå¼€å§‹çˆ¬å–:http://www.ip3366.net/?stype=1')
        total_pages = 7
        error_count = 0
        for page in range(1, total_pages + 1):
            proxy_list = ProxyScraper(f'http://www.ip3366.net/?stype=1&page={page}',
                            '<tr>.*?<td>(?P<ip>.*?)</td>.*?<td>(?P<port>.*?)</td>.*?</tr>', ['ip', 'port']).scrape_proxies()
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1

            time.sleep(1)

            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            percent = page * 100 // total_pages
            # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
            completed = page * 50 // total_pages
            remaining = 50 - completed
            # å¤„ç†ç™¾åˆ†æ¯”æ˜¾ç¤ºçš„å¯¹é½
            if percent < 10:
                padding = "  "
            elif percent < 100:
                padding = " "
            else:
                padding = ""
            # æ›´æ–°è¿›åº¦æ¡
            print(f"\r{percent}%{padding}|{'â–ˆ' * completed}{'-' * remaining}| {page}/{total_pages}  é”™è¯¯æ•°:{error_count}", end="")
            sys.stdout.flush()
        print('\n')

    elif scraper_choice == "6":
        try:
            by_type = 'http'   # é»˜è®¤ç”¨http
            count = int(input("çˆ¬å–ä¸ªæ•°(æ•´æ•°)ï¼š").strip())
            if count < 1:
                print("æ•°é‡å¿…é¡»å¤§äº0")
                return None,None

            print(f"\nå¼€å§‹çˆ¬å– {count} ä¸ªä»£ç†...")
        #     try:
        #         import aiohttp   # åç¨‹
        #         import asyncio
        #         semaphore = asyncio.Semaphore(20)  # æœ€å¤§å¹¶å‘
        #         timeout = aiohttp.ClientTimeout(total=50)  # è¶…æ—¶(ç»™æœåŠ¡å™¨è¶³å¤Ÿå“åº”æ—¶é—´)
        #         async def fetch_proxy(url):
                    
        #             async with aiohttp.ClientSession(timeout=timeout) as session:
        #                 async with session.get(url=url) as response:
        #                     if response.status == 200:
        #                         proxy = await response.text()
        #                         print(proxy)
        #                         return proxy
                
        #         async def fetch_proxies_main():
        #             url = 'https://proxypool.scrape.center/random'
        #             async with semaphore:
        #                 tasks = [asyncio.create_task(fetch_proxy(url=url)) for _ in range(count)]
        #                 results  = await asyncio.gather(*tasks, return_exceptions=True)
                    
        #             return results
        #         # proxy = requests.get('https://proxypool.scrape.center/random', timeout=timeout).text.strip()
        #         proxies = asyncio.run(fetch_proxies_main())
        #         if len(proxies) > 0:
        #             all_proxies = proxies
        #             # print(all_proxies)
        #     except:
        #         pass

        #     print("\nçˆ¬å–å®Œæˆï¼")
        # except:
        #     print("è¾“å…¥é”™è¯¯")
            try:
                # é€‚åˆç”¨åç¨‹
                import aiohttp
                import asyncio
                
                async def fetch_proxy(session, url, semaphore):
                    async with semaphore:
                        try:
                            async with session.get(url) as response:
                                if response.status == 200:
                                    proxy = await response.text()
                                    print(proxy)
                                    return proxy.strip()
                        except:
                            return None
                
                async def fetch_proxies_main():
                    semaphore = asyncio.Semaphore(20)   # æœ€å¤§å¹¶å‘
                    timeout = aiohttp.ClientTimeout(total=50)   # è¶…æ—¶(ç»™æœåŠ¡å™¨è¶³å¤Ÿå“åº”æ—¶é—´)
                    
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        tasks = []
                        for _ in range(count):
                            url = 'https://proxypool.scrape.center/random'
                            task = fetch_proxy(session, url, semaphore)
                            tasks.append(task)
                        
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        return [r for r in results if r and isinstance(r, str) and ':' in r]
                
                proxies = asyncio.run(fetch_proxies_main())
                if proxies:
                    all_proxies.extend(proxies)
                    
            except ImportError:
                print("aiohttp æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥è¯·æ±‚...")
                # åŒæ­¥å¤‡é€‰æ–¹æ¡ˆ
                for _ in range(count):
                    try:
                        proxy = requests.get('https://proxypool.scrape.center/random', timeout=30).text.strip()
                        if proxy and ':' in proxy:
                            all_proxies.append(proxy)
                    except:
                        continue

            print(f"\nçˆ¬å–å®Œæˆï¼")
        except ValueError:
            print("è¾“å…¥é”™è¯¯")
            return None, None

    elif scraper_choice == '7':
        print("\nå¼€å§‹çˆ¬å–:https://proxy.scdn.io/text.php")
        error_count = 0
        url = 'https://proxy.scdn.io/text.php'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
            'Referer':'https://proxy.scdn.io/'
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')
    
    elif scraper_choice == '8':
        print('\nå¼€å§‹çˆ¬å–:https://proxyhub.me/zh/cn-http-proxy-list.html')
        error_count = 0
        proxy_list = ProxyScraper("https://proxyhub.me/zh/cn-http-proxy-list.html",
                        r'<tr>\s*<td>(?P<ip>\d+\.\d+\.\d+\.\d+)</td>\s*<td>(?P<port>\d+)</td>',
                        ["ip", "port"]).scrape_proxies()
        if isinstance(proxy_list, list):
            all_proxies.extend(proxy_list)
        else:
            error_count += 1
        print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')


    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt
    elif scraper_choice == '9':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '10':
        by_type = 'socks5'   # é»˜è®¤ç”¨socks5
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '11':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(proxy) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # TODO https://github.com/zloi-user/hideip.me/raw/refs/heads/master/http.txt
    # TODO https://github.com/zloi-user/hideip.me/raw/refs/heads/master/https.txt
    # TODO https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks4.txt
    # TODO https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks5.txt


    # TODO https://raw.githubusercontent.com/r00tee/Proxy-List/main/Https.txt
    # TODO https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks4.txt
    # TODO https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt

    else:   # é€€å‡º
        print("é€€å‡º")
        return None, None
    
    return filter_proxies(all_proxies), by_type   # è¿”å›ç­›é€‰åçš„ä»£ç†åˆ—è¡¨,æ˜¯å¦æŒ‡å®šç±»å‹å’ŒæŒ‡å®šçš„ç±»å‹

def extract_proxies_by_type(num, proxy_type="all"):
    """
    æŒ‰ç±»å‹æå–æŒ‡å®šæ•°é‡çš„ä»£ç†ï¼Œä¼˜å…ˆæå–åˆ†é«˜çš„
    
    :param num: æ•°é‡
    :param proxy_type: ä»£ç†ç±»å‹ - "http", "socks4", "socks5", "all"
    :return: ä»£ç†åˆ—è¡¨
    """
    proxies, proxy_types = load_proxies_from_file(OUTPUT_FILE)
    
    # æŒ‰ç±»å‹ç­›é€‰
    if proxy_type != "all":
        filtered_proxies = {k: v for k, v in proxies.items() 
                          if proxy_types.get(k) == proxy_type}
    else:
        filtered_proxies = proxies

    # æŒ‰åˆ†æ•°é™åºæ’åº
    sorted_proxies = sorted(filtered_proxies.items(), key=lambda x: x[1], reverse=True)

    result = []
    for proxy, score in sorted_proxies:
        if len(result) >= num:
            break
        actual_type = proxy_types.get(proxy, "http")
        result.append(f"{actual_type}://{proxy}")

    return result

def extract_proxies_menu():
    """æå–ä»£ç†èœå•ï¼ˆæ”¯æŒæŒ‰ç±»å‹ç­›é€‰ï¼‰"""
    try:
        count = int(input("è¯·è¾“å…¥è¦æå–çš„ä»£ç†æ•°é‡: ").strip())
        if count <= 0:
            print("æ•°é‡å¿…é¡»å¤§äº0")
            return

        # é€‰æ‹©ä»£ç†ç±»å‹
        print("\né€‰æ‹©ä»£ç†ç±»å‹:")
        print("1. http/https")
        print("2. socks4")
        print("3. socks5")
        print("4. å…¨éƒ¨ç±»å‹")
        type_choice = input("è¯·é€‰æ‹©(1-4): ").strip()
        
        type_map = {
            "1": "http",
            "2": "socks4", 
            "3": "socks5",
            "4": "all"
        }
        
        proxy_type = type_map.get(type_choice, "all")
        
        proxies = extract_proxies_by_type(count, proxy_type)
        if not proxies:
            print("ä»£ç†æ± ä¸­æ²¡æœ‰å¯ç”¨ä»£ç†")
            return

        if len(proxies) < count:
            print(f"âš ï¸ è­¦å‘Š: åªæœ‰ {len(proxies)} ä¸ªå¯ç”¨ä»£ç†ï¼Œå°‘äºè¯·æ±‚çš„ {count} ä¸ª")

        print(f"\næå–çš„ä»£ç†åˆ—è¡¨({proxy_type}):")
        for i, proxy in enumerate(proxies, 1):
            print(f"{i}. {proxy}")

        save_choice = input("æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶? (y/n): ").lower().strip()
        if save_choice == "y":
            filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ")
            with open(filename, "w", encoding="utf-8") as file:
                for proxy in proxies:
                    file.write(f"{proxy}\n")
            print(f"å·²ä¿å­˜åˆ° {filename}")
    except ValueError:
        print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

def show_proxy_pool_status():
    """æ˜¾ç¤ºä»£ç†æ± çŠ¶æ€ï¼ˆæŒ‰ç±»å‹å’Œåˆ†æ•°ç»Ÿè®¡ï¼‰"""
    proxies, proxy_types = load_proxies_from_file(OUTPUT_FILE)
    total = len(proxies)
    
    if total == 0:
        print("ä»£ç†æ± ä¸ºç©º")
        return

    # æŒ‰ç±»å‹åˆ†ç»„
    type_groups = {}
    for proxy, score in proxies.items():
        proxy_type = proxy_types.get(proxy, "http")
        if proxy_type not in type_groups:
            type_groups[proxy_type] = []
        type_groups[proxy_type].append((proxy, score))

    print(f"\nä»£ç†æ± çŠ¶æ€ ({OUTPUT_FILE}):")
    print(f"æ€»ä»£ç†æ•°é‡: {total}")
    
    # æŒ‰ç±»å‹æ˜¾ç¤ºç»Ÿè®¡
    for proxy_type, proxy_list in type_groups.items():
        type_count = len(proxy_list)
        print(f"\n{proxy_type.upper()} ä»£ç†: {type_count}ä¸ª")
        
        # ç»Ÿè®¡åˆ†æ•°åˆ†å¸ƒ
        score_count = {}
        for _, score in proxy_list:
            score_count[score] = score_count.get(score, 0) + 1
        
        # æŒ‰åˆ†æ•°æ’åºæ˜¾ç¤º
        sorted_scores = sorted(score_count.items(), key=lambda x: x[0], reverse=True)
        for score, count in sorted_scores:
            print(f"  {score}åˆ†: {count}ä¸ª")
        
    print('='*40)
    print(f'æ€»è®¡: {total} ä¸ªä»£ç†')
def load_from_csv_with_type():
    """ä»CSVæ–‡ä»¶åŠ è½½å¹¶éªŒè¯ä»£ç†ï¼ˆæ”¯æŒç±»å‹é€‰æ‹©ï¼Œæ·»åŠ ä¸­æ–­æ¢å¤ï¼‰"""
    try:
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è®°å½•
        remaining_proxies, proxy_type, original_count = load_interrupted_proxies(INTERRUPT_FILE_LOAD)
        if remaining_proxies:
            print(f"ğŸ” å‘ç°ä¸Šæ¬¡æ–‡ä»¶åŠ è½½ä¸­æ–­è®°å½•!")
            print(f"   å‰©ä½™ä»£ç†: {len(remaining_proxies)}/{original_count} ä¸ª")
            print(f"   éªŒè¯ç±»å‹: {proxy_type}")
            print("\nè¯·é€‰æ‹©:")
            print("  y: ç»§ç»­ä¸Šæ¬¡éªŒè¯")
            print("  n: åˆ é™¤è®°å½•å¹¶é‡æ–°é€‰æ‹©æ–‡ä»¶")
            print("  å…¶ä»–: è¿”å›ä¸Šçº§èœå•")
            
            choice = input("è¯·é€‰æ‹© (y/n/å…¶ä»–): ").lower().strip()
            
            if choice == 'y':
                print("ç»§ç»­ä¸Šæ¬¡éªŒè¯...")
                validate_new_proxies_with_interrupt(remaining_proxies, proxy_type, from_interrupt=True, source="load")
                return
            elif choice == 'n':
                delete_interrupt_file(INTERRUPT_FILE_LOAD)
                print("å·²åˆ é™¤ä¸­æ–­è®°å½•ï¼Œå¼€å§‹é‡æ–°é€‰æ‹©æ–‡ä»¶...")
            else:
                print("è¿”å›ä¸Šçº§èœå•")
                return

        filename = input('æ–‡ä»¶å(è·¯å¾„): ')
        if not os.path.exists(filename):
            print("æ–‡ä»¶ä¸å­˜åœ¨")
            return
            
        # é€‰æ‹©ä»£ç†ç±»å‹
        print("\né€‰æ‹©ä»£ç†ç±»å‹:")
        print("1. http/https")
        print("2. socks4")
        print("3. socks5")
        print("4. è‡ªåŠ¨æ£€æµ‹")
        print("è¾“å…¥å…¶ä»–: ä½¿ç”¨é»˜è®¤å€¼http")
        type_choice = input("è¯·é€‰æ‹©(1-4): ").strip()
        
        type_map = {
            "1": "http",
            "2": "socks4",
            "3": "socks5",
            "4": "auto"
        }
        
        selected_type = type_map.get(type_choice, "http")
        print(f"ä½¿ç”¨ç±»å‹: {selected_type}")
        
        data = []
        with open(filename, 'r', encoding='utf-8') as file:
            reader = csv.reader(file)
            for row in reader:
                if len(row) >= 2:
                    # æ”¯æŒ ip,port æ ¼å¼
                    ip = row[0].strip()
                    port = row[1].strip()
                    if ip and port:
                        data.append(f"{ip}:{port}")
                elif len(row) == 1 and ':' in row[0]:
                    # æ”¯æŒ ip:port æ ¼å¼
                    data.append(row[0].strip())
        
        if not data:
            print("æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä»£ç†")
            return
            
        print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(data)} ä¸ªä»£ç†")
        
        # ç­›é€‰å»é‡
        new_proxies = filter_proxies(data)
        
        if new_proxies:
            if selected_type != "auto":
                # ä½¿ç”¨æŒ‡å®šç±»å‹éªŒè¯
                validate_new_proxies_with_interrupt(new_proxies, selected_type, source="load")
            else:
                # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
                validate_new_proxies_with_interrupt(new_proxies, "auto", source="load")
        else:
            print("æ²¡æœ‰æ–°ä»£ç†éœ€è¦éªŒè¯")
            
    except Exception as e:
        print(f'å‡ºé”™äº†: {str(e)}')


if __name__ == '__main__':
    # åˆ›å»ºä¸­æ–­ç›®å½•
    create_interrupt_dir()

    while True:
        print(f"""åŠŸèƒ½ï¼š
        1: åŠ è½½å¹¶éªŒè¯æ–°ä»£ç† (æˆåŠŸåæ·»åŠ åˆ°ä»£ç†æ± )
        2: æ£€éªŒå¹¶æ›´æ–°å·²æœ‰ä»£ç†
        3: æå–ä»£ç†(å¯æŒ‡å®šæ•°é‡,ç±»å‹)
        4: æŸ¥çœ‹ä»£ç†æ± çŠ¶æ€


        è¾“å…¥å…¶ä»–: é€€å‡º
        """)
        choice = input("é€‰æ‹©ï¼š").strip()

        if choice == "1":
            print('''æ¥è‡ª:
                  1: æ¥è‡ªçˆ¬è™«çˆ¬å–
                  2: æ¥è‡ªæœ¬åœ°æ–‡ä»¶(proxy,port)

                  è¾“å…¥å…¶ä»–: è¿”å›ä¸Šçº§èœå•
            ''')
            from_choice = input('é€‰æ‹©:').strip()

            if from_choice == '1':
                new_proxies, by_type = crawl_proxies()
                if new_proxies:   # å¦‚æœæœ‰æ–°ä»£ç†
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä»ä¸­æ–­æ¢å¤çš„
                    remaining_proxies, interrupt_type, _ = load_interrupted_proxies()
                    from_interrupt = remaining_proxies is not None and remaining_proxies == new_proxies
                    
                    if by_type:   # å¦‚æœæŒ‡å®šç±»å‹
                        validate_new_proxies_with_interrupt(new_proxies, by_type, from_interrupt)
                    else:   # æ²¡æœ‰æŒ‡å®šç±»å‹
                        validate_new_proxies_with_interrupt(new_proxies, "auto", from_interrupt)

            elif from_choice == '2':
                load_from_csv_with_type()

            else:
                print('è¿”å›ä¸Šçº§èœå•')
                continue

        elif choice == "2":
            validate_existing_proxies_with_interrupt()

        elif choice == "3":
            extract_proxies_menu()

        elif choice == "4":
            show_proxy_pool_status()

        else:
            print('é€€å‡º')
            break
